{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-20T18:20:16.491443Z",
     "start_time": "2024-09-20T18:20:16.189703Z"
    }
   },
   "source": [
    "#Load and preprocess the data from both takehome_user_engagement.csv and takehome_users.csv files\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# Try different encodings\n",
    "encodings = ['utf-8', 'iso-8859-1', 'cp1252']\n",
    "\n",
    "for encoding in encodings:\n",
    "    try:\n",
    "        users_df = pd.read_csv('takehome_users.csv', encoding=encoding, parse_dates=['creation_time', 'last_session_creation_time'])\n",
    "        engagement_df = pd.read_csv('takehome_user_engagement.csv', encoding=encoding, parse_dates=['time_stamp'])\n",
    "        print(f\"Successfully read files with encoding: {encoding}\")\n",
    "        break\n",
    "    except UnicodeDecodeError:\n",
    "        print(f\"Failed to read with encoding: {encoding}\")\n",
    "\n",
    "# Continue with preprocessing if files were successfully read\n",
    "if 'users_df' in locals() and 'engagement_df' in locals():\n",
    "    # Preprocess users dataframe\n",
    "    users_df['creation_time'] = pd.to_datetime(users_df['creation_time'])\n",
    "    users_df['last_session_creation_time'] = pd.to_datetime(users_df['last_session_creation_time'])\n",
    "    users_df['days_since_creation'] = (datetime.now() - users_df['creation_time']).dt.days\n",
    "    users_df['days_since_last_session'] = (datetime.now() - users_df['last_session_creation_time']).dt.days\n",
    "\n",
    "    # Preprocess engagement dataframe\n",
    "    engagement_df['time_stamp'] = pd.to_datetime(engagement_df['time_stamp'])\n",
    "\n",
    "    # Display info about the dataframes\n",
    "    print(users_df.info())\n",
    "    print(\"\\n\")\n",
    "    print(engagement_df.info())\n",
    "\n",
    "    # Display the first few rows of each dataframe\n",
    "    print(\"\\nUsers DataFrame:\")\n",
    "    print(users_df.head())\n",
    "    print(\"\\nEngagement DataFrame:\")\n",
    "    print(engagement_df.head())\n",
    "else:\n",
    "    print(\"Failed to read CSV files with available encodings.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to read with encoding: utf-8\n",
      "Successfully read files with encoding: iso-8859-1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Katarina\\AppData\\Local\\Temp\\ipykernel_3580\\2233870967.py:11: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  users_df = pd.read_csv('takehome_users.csv', encoding=encoding, parse_dates=['creation_time', 'last_session_creation_time'])\n",
      "C:\\Users\\Katarina\\AppData\\Local\\Temp\\ipykernel_3580\\2233870967.py:22: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  users_df['last_session_creation_time'] = pd.to_datetime(users_df['last_session_creation_time'])\n"
     ]
    },
    {
     "ename": "DateParseError",
     "evalue": "year 1398138810 is out of range: 1398138810, at position 0",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "File \u001B[1;32mparsing.pyx:684\u001B[0m, in \u001B[0;36mpandas._libs.tslibs.parsing.dateutil_parse\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;31mValueError\u001B[0m: year 1398138810 is out of range",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mDateParseError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 22\u001B[0m\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124musers_df\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mlocals\u001B[39m() \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mengagement_df\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mlocals\u001B[39m():\n\u001B[0;32m     20\u001B[0m     \u001B[38;5;66;03m# Preprocess users dataframe\u001B[39;00m\n\u001B[0;32m     21\u001B[0m     users_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcreation_time\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mto_datetime(users_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcreation_time\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m---> 22\u001B[0m     users_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlast_session_creation_time\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto_datetime\u001B[49m\u001B[43m(\u001B[49m\u001B[43musers_df\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mlast_session_creation_time\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     23\u001B[0m     users_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdays_since_creation\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m (datetime\u001B[38;5;241m.\u001B[39mnow() \u001B[38;5;241m-\u001B[39m users_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcreation_time\u001B[39m\u001B[38;5;124m'\u001B[39m])\u001B[38;5;241m.\u001B[39mdt\u001B[38;5;241m.\u001B[39mdays\n\u001B[0;32m     24\u001B[0m     users_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdays_since_last_session\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m (datetime\u001B[38;5;241m.\u001B[39mnow() \u001B[38;5;241m-\u001B[39m users_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlast_session_creation_time\u001B[39m\u001B[38;5;124m'\u001B[39m])\u001B[38;5;241m.\u001B[39mdt\u001B[38;5;241m.\u001B[39mdays\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\tools\\datetimes.py:1067\u001B[0m, in \u001B[0;36mto_datetime\u001B[1;34m(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)\u001B[0m\n\u001B[0;32m   1065\u001B[0m         result \u001B[38;5;241m=\u001B[39m arg\u001B[38;5;241m.\u001B[39mmap(cache_array)\n\u001B[0;32m   1066\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1067\u001B[0m         values \u001B[38;5;241m=\u001B[39m \u001B[43mconvert_listlike\u001B[49m\u001B[43m(\u001B[49m\u001B[43marg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_values\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mformat\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1068\u001B[0m         result \u001B[38;5;241m=\u001B[39m arg\u001B[38;5;241m.\u001B[39m_constructor(values, index\u001B[38;5;241m=\u001B[39marg\u001B[38;5;241m.\u001B[39mindex, name\u001B[38;5;241m=\u001B[39marg\u001B[38;5;241m.\u001B[39mname)\n\u001B[0;32m   1069\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(arg, (ABCDataFrame, abc\u001B[38;5;241m.\u001B[39mMutableMapping)):\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\tools\\datetimes.py:435\u001B[0m, in \u001B[0;36m_convert_listlike_datetimes\u001B[1;34m(arg, format, name, utc, unit, errors, dayfirst, yearfirst, exact)\u001B[0m\n\u001B[0;32m    432\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mformat\u001B[39m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mformat\u001B[39m \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmixed\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    433\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _array_strptime_with_fallback(arg, name, utc, \u001B[38;5;28mformat\u001B[39m, exact, errors)\n\u001B[1;32m--> 435\u001B[0m result, tz_parsed \u001B[38;5;241m=\u001B[39m \u001B[43mobjects_to_datetime64\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    436\u001B[0m \u001B[43m    \u001B[49m\u001B[43marg\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    437\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdayfirst\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdayfirst\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    438\u001B[0m \u001B[43m    \u001B[49m\u001B[43myearfirst\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43myearfirst\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    439\u001B[0m \u001B[43m    \u001B[49m\u001B[43mutc\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mutc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    440\u001B[0m \u001B[43m    \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    441\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_object\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    442\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    444\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m tz_parsed \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    445\u001B[0m     \u001B[38;5;66;03m# We can take a shortcut since the datetime64 numpy array\u001B[39;00m\n\u001B[0;32m    446\u001B[0m     \u001B[38;5;66;03m# is in UTC\u001B[39;00m\n\u001B[0;32m    447\u001B[0m     out_unit \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mdatetime_data(result\u001B[38;5;241m.\u001B[39mdtype)[\u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\arrays\\datetimes.py:2398\u001B[0m, in \u001B[0;36mobjects_to_datetime64\u001B[1;34m(data, dayfirst, yearfirst, utc, errors, allow_object, out_unit)\u001B[0m\n\u001B[0;32m   2395\u001B[0m \u001B[38;5;66;03m# if str-dtype, convert\u001B[39;00m\n\u001B[0;32m   2396\u001B[0m data \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39masarray(data, dtype\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39mobject_)\n\u001B[1;32m-> 2398\u001B[0m result, tz_parsed \u001B[38;5;241m=\u001B[39m \u001B[43mtslib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43marray_to_datetime\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   2399\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2400\u001B[0m \u001B[43m    \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2401\u001B[0m \u001B[43m    \u001B[49m\u001B[43mutc\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mutc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2402\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdayfirst\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdayfirst\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2403\u001B[0m \u001B[43m    \u001B[49m\u001B[43myearfirst\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43myearfirst\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2404\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreso\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mabbrev_to_npy_unit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mout_unit\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2405\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2407\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m tz_parsed \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   2408\u001B[0m     \u001B[38;5;66;03m# We can take a shortcut since the datetime64 numpy array\u001B[39;00m\n\u001B[0;32m   2409\u001B[0m     \u001B[38;5;66;03m#  is in UTC\u001B[39;00m\n\u001B[0;32m   2410\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m result, tz_parsed\n",
      "File \u001B[1;32mtslib.pyx:414\u001B[0m, in \u001B[0;36mpandas._libs.tslib.array_to_datetime\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mtslib.pyx:596\u001B[0m, in \u001B[0;36mpandas._libs.tslib.array_to_datetime\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mtslib.pyx:553\u001B[0m, in \u001B[0;36mpandas._libs.tslib.array_to_datetime\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mconversion.pyx:641\u001B[0m, in \u001B[0;36mpandas._libs.tslibs.conversion.convert_str_to_tsobject\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mparsing.pyx:336\u001B[0m, in \u001B[0;36mpandas._libs.tslibs.parsing.parse_datetime_string\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mparsing.pyx:688\u001B[0m, in \u001B[0;36mpandas._libs.tslibs.parsing.dateutil_parse\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;31mDateParseError\u001B[0m: year 1398138810 is out of range: 1398138810, at position 0"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T18:22:16.092852Z",
     "start_time": "2024-09-20T18:22:07.395877Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def is_user_adopted(user_data):\n",
    "    # Sort the user's engagement data by timestamp\n",
    "    user_data = user_data.sort_values('time_stamp')\n",
    "    \n",
    "    # Convert timestamps to datetime if they aren't already\n",
    "    user_data['time_stamp'] = pd.to_datetime(user_data['time_stamp'])\n",
    "    \n",
    "    # Calculate the difference in days between consecutive logins\n",
    "    user_data['days_diff'] = user_data['time_stamp'].diff().dt.days\n",
    "    \n",
    "    # Create a rolling 7-day window\n",
    "    rolling_window = user_data['days_diff'].rolling(window=7)\n",
    "    \n",
    "    # Count unique days in each 7-day window\n",
    "    unique_days_in_window = rolling_window.apply(lambda x: x.nunique())\n",
    "    \n",
    "    # Check if any 7-day window has at least 3 unique days\n",
    "    return (unique_days_in_window >= 3).any()\n",
    "\n",
    "# Apply the function to the engagement data grouped by user\n",
    "engagement_df['is_adopted'] = engagement_df.groupby('user_id').apply(is_user_adopted)\n",
    "\n",
    "print(engagement_df.head())\n",
    "print(engagement_df['is_adopted'].value_counts())"
   ],
   "id": "25fc354bc3ab31bd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           time_stamp  user_id  visited is_adopted\n",
      "0 2014-04-22 03:53:30        1        1        NaN\n",
      "1 2013-11-15 03:45:04        2        1      False\n",
      "2 2013-11-29 03:45:04        2        1       True\n",
      "3 2013-12-09 03:45:04        2        1      False\n",
      "4 2013-12-25 03:45:04        2        1      False\n",
      "is_adopted\n",
      "False    7260\n",
      "True     1563\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Katarina\\AppData\\Local\\Temp\\ipykernel_3580\\570185930.py:21: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  engagement_df['is_adopted'] = engagement_df.groupby('user_id').apply(is_user_adopted)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This data gives a good starting point for further analysis.  From the above, it can be seen that around 17.7% of users in the dataset are considered adopted based on criteria.",
   "id": "edf019b605db67d2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T18:28:01.497274Z",
     "start_time": "2024-09-20T18:28:01.438288Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Merge the engagement data with user information\n",
    "merged_df = pd.merge(engagement_df, users_df, left_on='user_id', right_on='object_id', how='left')\n",
    "\n",
    "# Drop duplicate columns if any\n",
    "merged_df = merged_df.loc[:,~merged_df.columns.duplicated()]\n",
    "\n",
    "# Display info about the merged dataframe\n",
    "print(merged_df.info())\n",
    "\n",
    "# Display the first few rows of the merged dataframe\n",
    "print(\"\\nMerged DataFrame:\")\n",
    "print(merged_df.head())\n",
    "\n",
    "# Check the number of rows before and after merging\n",
    "print(f\"\\nNumber of rows in engagement_df: {len(engagement_df)}\")\n",
    "print(f\"Number of rows in users_df: {len(users_df)}\")\n",
    "print(f\"Number of rows in merged_df: {len(merged_df)}\")"
   ],
   "id": "9ea518f10f94c4fc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 207917 entries, 0 to 207916\n",
      "Data columns (total 14 columns):\n",
      " #   Column                      Non-Null Count   Dtype         \n",
      "---  ------                      --------------   -----         \n",
      " 0   time_stamp                  207917 non-null  datetime64[ns]\n",
      " 1   user_id                     207917 non-null  int64         \n",
      " 2   visited                     207917 non-null  int64         \n",
      " 3   is_adopted                  8823 non-null    object        \n",
      " 4   object_id                   207917 non-null  int64         \n",
      " 5   creation_time               207917 non-null  datetime64[ns]\n",
      " 6   name                        207917 non-null  object        \n",
      " 7   email                       207917 non-null  object        \n",
      " 8   creation_source             207917 non-null  object        \n",
      " 9   last_session_creation_time  207917 non-null  object        \n",
      " 10  opted_in_to_mailing_list    207917 non-null  int64         \n",
      " 11  enabled_for_marketing_drip  207917 non-null  int64         \n",
      " 12  org_id                      207917 non-null  int64         \n",
      " 13  invited_by_user_id          116887 non-null  float64       \n",
      "dtypes: datetime64[ns](2), float64(1), int64(6), object(5)\n",
      "memory usage: 22.2+ MB\n",
      "None\n",
      "\n",
      "Merged DataFrame:\n",
      "           time_stamp  user_id  visited is_adopted  object_id  \\\n",
      "0 2014-04-22 03:53:30        1        1        NaN          1   \n",
      "1 2013-11-15 03:45:04        2        1      False          2   \n",
      "2 2013-11-29 03:45:04        2        1       True          2   \n",
      "3 2013-12-09 03:45:04        2        1      False          2   \n",
      "4 2013-12-25 03:45:04        2        1      False          2   \n",
      "\n",
      "        creation_time            name                     email  \\\n",
      "0 2014-04-22 03:53:30  Clausen August  AugustCClausen@yahoo.com   \n",
      "1 2013-11-15 03:45:04   Poole Matthew    MatthewPoole@gustr.com   \n",
      "2 2013-11-15 03:45:04   Poole Matthew    MatthewPoole@gustr.com   \n",
      "3 2013-11-15 03:45:04   Poole Matthew    MatthewPoole@gustr.com   \n",
      "4 2013-11-15 03:45:04   Poole Matthew    MatthewPoole@gustr.com   \n",
      "\n",
      "  creation_source last_session_creation_time  opted_in_to_mailing_list  \\\n",
      "0    GUEST_INVITE                 1398138810                         1   \n",
      "1      ORG_INVITE                 1396237504                         0   \n",
      "2      ORG_INVITE                 1396237504                         0   \n",
      "3      ORG_INVITE                 1396237504                         0   \n",
      "4      ORG_INVITE                 1396237504                         0   \n",
      "\n",
      "   enabled_for_marketing_drip  org_id  invited_by_user_id  \n",
      "0                           0      11             10803.0  \n",
      "1                           0       1               316.0  \n",
      "2                           0       1               316.0  \n",
      "3                           0       1               316.0  \n",
      "4                           0       1               316.0  \n",
      "\n",
      "Number of rows in engagement_df: 207917\n",
      "Number of rows in users_df: 12000\n",
      "Number of rows in merged_df: 207917\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T18:29:06.099035Z",
     "start_time": "2024-09-20T18:29:06.004676Z"
    }
   },
   "cell_type": "code",
   "source": [
    "merged_df['creation_time'] = pd.to_datetime(merged_df['creation_time'])\n",
    "merged_df['account_age_days'] = (merged_df['time_stamp'] - merged_df['creation_time']).dt.days\n",
    "merged_df['is_invited'] = merged_df['invited_by_user_id'].notna().astype(int)\n",
    "merged_df['creation_source_encoded'] = pd.Categorical(merged_df['creation_source']).codes\n",
    "merged_df['total_logins'] = merged_df.groupby('user_id')['visited'].transform('sum')\n",
    "merged_df['avg_logins_per_day'] = merged_df['total_logins'] / (merged_df['account_age_days'] + 1)\n",
    "merged_df['days_since_last_login'] = (merged_df.groupby('user_id')['time_stamp'].transform('max') - merged_df['time_stamp']).dt.days\n",
    "\n",
    "# dummy variables for creation_source\n",
    "creation_source_dummies = pd.get_dummies(merged_df['creation_source'], prefix='source')\n",
    "merged_df = pd.concat([merged_df, creation_source_dummies], axis=1)\n",
    "print(merged_df.info())\n",
    "\n",
    "print(\"\\nUpdated Merged DataFrame:\")\n",
    "print(merged_df.head())"
   ],
   "id": "a50f01554f3c52a8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 207917 entries, 0 to 207916\n",
      "Data columns (total 25 columns):\n",
      " #   Column                      Non-Null Count   Dtype         \n",
      "---  ------                      --------------   -----         \n",
      " 0   time_stamp                  207917 non-null  datetime64[ns]\n",
      " 1   user_id                     207917 non-null  int64         \n",
      " 2   visited                     207917 non-null  int64         \n",
      " 3   is_adopted                  8823 non-null    object        \n",
      " 4   object_id                   207917 non-null  int64         \n",
      " 5   creation_time               207917 non-null  datetime64[ns]\n",
      " 6   name                        207917 non-null  object        \n",
      " 7   email                       207917 non-null  object        \n",
      " 8   creation_source             207917 non-null  object        \n",
      " 9   last_session_creation_time  207917 non-null  object        \n",
      " 10  opted_in_to_mailing_list    207917 non-null  int64         \n",
      " 11  enabled_for_marketing_drip  207917 non-null  int64         \n",
      " 12  org_id                      207917 non-null  int64         \n",
      " 13  invited_by_user_id          116887 non-null  float64       \n",
      " 14  account_age_days            207917 non-null  int64         \n",
      " 15  is_invited                  207917 non-null  int32         \n",
      " 16  creation_source_encoded     207917 non-null  int8          \n",
      " 17  total_logins                207917 non-null  int64         \n",
      " 18  avg_logins_per_day          207917 non-null  float64       \n",
      " 19  days_since_last_login       207917 non-null  int64         \n",
      " 20  source_GUEST_INVITE         207917 non-null  bool          \n",
      " 21  source_ORG_INVITE           207917 non-null  bool          \n",
      " 22  source_PERSONAL_PROJECTS    207917 non-null  bool          \n",
      " 23  source_SIGNUP               207917 non-null  bool          \n",
      " 24  source_SIGNUP_GOOGLE_AUTH   207917 non-null  bool          \n",
      "dtypes: bool(5), datetime64[ns](2), float64(2), int32(1), int64(9), int8(1), object(5)\n",
      "memory usage: 30.5+ MB\n",
      "None\n",
      "\n",
      "Updated Merged DataFrame:\n",
      "           time_stamp  user_id  visited is_adopted  object_id  \\\n",
      "0 2014-04-22 03:53:30        1        1        NaN          1   \n",
      "1 2013-11-15 03:45:04        2        1      False          2   \n",
      "2 2013-11-29 03:45:04        2        1       True          2   \n",
      "3 2013-12-09 03:45:04        2        1      False          2   \n",
      "4 2013-12-25 03:45:04        2        1      False          2   \n",
      "\n",
      "        creation_time            name                     email  \\\n",
      "0 2014-04-22 03:53:30  Clausen August  AugustCClausen@yahoo.com   \n",
      "1 2013-11-15 03:45:04   Poole Matthew    MatthewPoole@gustr.com   \n",
      "2 2013-11-15 03:45:04   Poole Matthew    MatthewPoole@gustr.com   \n",
      "3 2013-11-15 03:45:04   Poole Matthew    MatthewPoole@gustr.com   \n",
      "4 2013-11-15 03:45:04   Poole Matthew    MatthewPoole@gustr.com   \n",
      "\n",
      "  creation_source last_session_creation_time  ...  is_invited  \\\n",
      "0    GUEST_INVITE                 1398138810  ...           1   \n",
      "1      ORG_INVITE                 1396237504  ...           1   \n",
      "2      ORG_INVITE                 1396237504  ...           1   \n",
      "3      ORG_INVITE                 1396237504  ...           1   \n",
      "4      ORG_INVITE                 1396237504  ...           1   \n",
      "\n",
      "   creation_source_encoded  total_logins  avg_logins_per_day  \\\n",
      "0                        0             1            1.000000   \n",
      "1                        1            14           14.000000   \n",
      "2                        1            14            0.933333   \n",
      "3                        1            14            0.560000   \n",
      "4                        1            14            0.341463   \n",
      "\n",
      "   days_since_last_login  source_GUEST_INVITE  source_ORG_INVITE  \\\n",
      "0                      0                 True              False   \n",
      "1                    136                False               True   \n",
      "2                    122                False               True   \n",
      "3                    112                False               True   \n",
      "4                     96                False               True   \n",
      "\n",
      "   source_PERSONAL_PROJECTS  source_SIGNUP  source_SIGNUP_GOOGLE_AUTH  \n",
      "0                     False          False                      False  \n",
      "1                     False          False                      False  \n",
      "2                     False          False                      False  \n",
      "3                     False          False                      False  \n",
      "4                     False          False                      False  \n",
      "\n",
      "[5 rows x 25 columns]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T18:31:05.246334Z",
     "start_time": "2024-09-20T18:31:02.301772Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Prepare the data\n",
    "features = ['account_age_days', 'is_invited', 'creation_source_encoded', 'total_logins', \n",
    "            'avg_logins_per_day', 'opted_in_to_mailing_list', 'enabled_for_marketing_drip']\n",
    "X = merged_df[features]\n",
    "y = merged_df['is_adopted'].fillna(False).astype(int)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Fit logistic regression model\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Print feature importance\n",
    "feature_importance = pd.DataFrame({'feature': features, 'importance': abs(model.coef_[0])})\n",
    "print(feature_importance.sort_values('importance', ascending=False))\n",
    "\n",
    "# Fit statsmodels logistic regression for p-values\n",
    "X_train_sm = sm.add_constant(X_train)\n",
    "sm_model = sm.Logit(y_train, X_train_sm)\n",
    "sm_results = sm_model.fit()\n",
    "print(sm_results.summary())\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "print(classification_report(y_test, y_pred))"
   ],
   "id": "daed586a2c638a28",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Katarina\\AppData\\Local\\Temp\\ipykernel_3580\\1916125988.py:11: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  y = merged_df['is_adopted'].fillna(False).astype(int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      feature  importance\n",
      "6  enabled_for_marketing_drip    0.202851\n",
      "1                  is_invited    0.154085\n",
      "3                total_logins    0.125513\n",
      "2     creation_source_encoded    0.097639\n",
      "0            account_age_days    0.096125\n",
      "4          avg_logins_per_day    0.051466\n",
      "5    opted_in_to_mailing_list    0.004420\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.044559\n",
      "         Iterations 10\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:             is_adopted   No. Observations:               166333\n",
      "Model:                          Logit   Df Residuals:                   166325\n",
      "Method:                           MLE   Df Model:                            7\n",
      "Date:                Fri, 20 Sep 2024   Pseudo R-squ.:                 0.01052\n",
      "Time:                        13:31:05   Log-Likelihood:                -7411.6\n",
      "converged:                       True   LL-Null:                       -7490.4\n",
      "Covariance Type:            nonrobust   LLR p-value:                 1.002e-30\n",
      "==============================================================================================\n",
      "                                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------------------\n",
      "const                         -4.6684      0.165    -28.237      0.000      -4.992      -4.344\n",
      "account_age_days              -0.0005      0.000     -2.595      0.009      -0.001      -0.000\n",
      "is_invited                     0.4106      0.135      3.031      0.002       0.145       0.676\n",
      "creation_source_encoded       -0.0267      0.048     -0.558      0.577      -0.120       0.067\n",
      "total_logins                  -0.0008      0.000     -3.390      0.001      -0.001      -0.000\n",
      "avg_logins_per_day            -0.0049      0.004     -1.210      0.226      -0.013       0.003\n",
      "opted_in_to_mailing_list       0.0413      0.073      0.568      0.570      -0.101       0.184\n",
      "enabled_for_marketing_drip    -0.6099      0.105     -5.799      0.000      -0.816      -0.404\n",
      "==============================================================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00     41298\n",
      "           1       0.00      0.00      0.00       286\n",
      "\n",
      "    accuracy                           0.99     41584\n",
      "   macro avg       0.50      0.50      0.50     41584\n",
      "weighted avg       0.99      0.99      0.99     41584\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Katarina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Katarina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Katarina\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Feature Importance) The most important feature is 'enabled_for_marketing_drip', followed by 'is_invited', and 'total_logins'.  'account_age_days' and 'creation_source_encoded' are also of moderate importance.  'avg_logins_per_day' and 'opted_in_to_mailing_list' are less important.  For statistical significance, 'enabled_for_marketing_drip' has a very low p-value (<0.0001), indicating strong statistical significance.  The model shows high accuracy (0.99) but performs poorly on predicting the positive class (adopted users). ",
   "id": "a970cc7f802d3de"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T18:37:44.373192Z",
     "start_time": "2024-09-20T18:37:36.784960Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "features = ['enabled_for_marketing_drip', 'is_invited', 'total_logins', \n",
    "            'account_age_days', 'creation_source_encoded', \n",
    "            'avg_logins_per_day', 'opted_in_to_mailing_list']\n",
    "X = merged_df[features]\n",
    "y = merged_df['is_adopted'].fillna(False).astype(int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Calculate class weights\n",
    "class_weights = {0: 1, 1: y_train.value_counts()[0] / y_train.value_counts()[1]}\n",
    "\n",
    "# Train Random Forest model with class weights\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight=class_weights)\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "feature_importance = pd.DataFrame({'feature': features, 'importance': rf_model.feature_importances_})\n",
    "print(\"Feature Importances:\")\n",
    "print(feature_importance.sort_values('importance', ascending=False))\n",
    "\n",
    "# Make predictions\n",
    "y_pred = rf_model.predict(X_test_scaled)\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ],
   "id": "ec5b47a29e7fedec",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Katarina\\AppData\\Local\\Temp\\ipykernel_3580\\3401605103.py:11: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  y = merged_df['is_adopted'].fillna(False).astype(int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importances:\n",
      "                      feature  importance\n",
      "2                total_logins    0.484245\n",
      "5          avg_logins_per_day    0.198543\n",
      "3            account_age_days    0.189806\n",
      "4     creation_source_encoded    0.064597\n",
      "6    opted_in_to_mailing_list    0.028144\n",
      "0  enabled_for_marketing_drip    0.017518\n",
      "1                  is_invited    0.017146\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99     41298\n",
      "           1       0.02      0.03      0.03       286\n",
      "\n",
      "    accuracy                           0.98     41584\n",
      "   macro avg       0.51      0.51      0.51     41584\n",
      "weighted avg       0.99      0.98      0.99     41584\n",
      "\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "'total_logins' is by far the most important feature, accounting for about 48% of the predictive power.  'avg_logins_per_day' and 'account_age_days' are the next most important features, each contributing around 19-20% to the model's decisions.  'creation_source_encoded' has moderate importance at about 6.5%.  And the remaining features ('opted_in_to_mailing_list', 'enabled_for_marketing_drip', and 'is_invited') have relatively lower importance, each contributing less than 3%.\n",
    "\n",
    "The data analysis had an objective to identify factors that predict future user adoption, which is defined as users who log into the product on three separate days in at least one seven-day period.  The methodology used for the analysis involved data preprocessing and feature engineering; logistic regression for initial insights; and the use of a random forest classifier with class weighting to address imbalance.  \n",
    "\n",
    "Key Findings were that user engagement metrics are the strongest predictors of adoption:  total logins (48.4%  importance), average logins per day (19.9% importance), and account age in days (19.0% importance); creation source has moderate predictive power (6.5% importance); and marketing-related features and invitation status have minimal impact on adoption prediction (each <3% importance).  The model performance had an overall accuracy of 98%, precision for adopted users 2%, and recall for adopted users 3%. \n",
    "\n",
    "Further analysis could be done to focus on login frequency and investigation of factors influencing total logins to develop strategies to encourage more frequent use.  Further analysis could look at the impact of different creation sources or even long-term adoption.  In conclusion, the analysis provides insight for improving user adoption rates by focusing on user behaviour patterns and key engagement metrics. "
   ],
   "id": "3957c59e2c11a6f3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "ea811dc3a4d50a4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
